[ LogisticRegression ]
# penalty = [ "newton-cg" ]
dual = false
C = 1.0
fit_intercept = true
intercept_scaling = 1
solver = [ "lbfgs" ] # "newton-cg", "sag", "saga", 
max_iter = 100
multi_class = [ "ovr", "multinomial", "auto" ]
l1_ratio = 0 # number from 0-1
random_state = 0

[ GaussianMixture ]
n_components = 1
covariance_type = ["full", "tied", "diag", "spherical"]
reg_covar = 1e-6
max_iter = 100
n_init = 1
random_state = 0

[ DecisionTreeClassifier ]
criterion = [ "gini", "entropy" ]
# using many random splitters here because it will try many random things
splitter = [ "best", "random", "random", "random", "random", "random", ]
max_depth = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ]
min_samples_leaf = [ 1, 2, 3, 4, 5 ]
min_weight_fraction_leaf = [ 0.0, 0.05, 0.10, 0.15, 0.20, 0.25 ]
max_features = [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 ]
max_leaf_nodes = [ 5, 10, 15, 20, 25 ]
min_impurity_decrease = [ 0.1, 0.2, 0.3, 0.4, 0.5 ]
presort = true
random_state = 0

[ RandomForestClassifier ]
n_estimators = [ 5, 10, 15, 20, 25 ]
criterion = [ "gini", "entropy" ]
max_depth = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ]
min_samples_leaf = [ 1, 2, 3, 4, 5 ]
min_weight_fraction_leaf = [ 0.0, 0.05, 0.10, 0.15, 0.20, 0.25 ]
max_features = [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 ]
max_leaf_nodes = [ 5, 10, 15, 20, 25 ]
min_impurity_decrease = [ 0.1, 0.2, 0.3, 0.4, 0.5 ]
random_state = 0

[ AdaBoostClassifier ]
n_estimators = [50, 75, 100, 125, 150]
learning_rate = [ 1.0, 0.9, 0.8, 0.7, 0.6, 0.5 ]
algorithm = [ "SAMME", "SAMME.R" ]
random_state = 0

[ GradientBoostingClassifier ]
loss = ["deviance", "exponential"]
learning_rate = [ 0.025, 0.05, 0.1, 0.2, 0.4 ]
n_estimators = [ 50, 100, 200, 300 ]
subsample = [ 0.5, 0.75, 1.0, 1.25, 1.5 ]
criterion = [ "friedman_mse", "mse", "mae" ]
min_samples_split = [ 2, 3, 4, 5 ]
max_depth = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ]
min_samples_leaf = [ 1, 2, 3, 4, 5 ]
min_weight_fraction_leaf = [ 0.0, 0.05, 0.10, 0.15, 0.20, 0.25 ]
max_features = [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100 ]
max_leaf_nodes = [ 5, 10, 15, 20, 25 ]
min_impurity_decrease = [ 0.1, 0.2, 0.3, 0.4, 0.5 ]
presort = true
validation_fraction = [ 0.1, 0.2, 0.4 ]
random_state = 0

[ KNeighborsClassifier ]
n_neighbors = [ 3, 5, 7, 9 ]
weights = [ "uniform", "distance" ]
algorithm = ["auto", "ball_tree", "kd_tree", "brute"  ]
leaf_size = [ 20, 25, 30, 35, 40 ]
p = [ 1, 2, 3, 4 ]
metric = "minkowski"
